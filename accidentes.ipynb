{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción y transformación de datos con Spark 2.4\n",
    "### Origen de datos\n",
    "Los datos representan los accidentes de tráfico por día y distrito\n",
    "que se han producido en la ciudad de Madrid entre 2010 y 2018\n",
    "\n",
    "El fichero origen es un fichero csv por año tiene los datos detallados\n",
    "por cada uno de los intervinientes en el accidente: Conductor, pasajero,\n",
    "testito, peatón, ...\n",
    "\n",
    "La primera parte del fichero está referida al accidente y se repite\n",
    "por cada uno de los intervinientes.\n",
    "\n",
    "### La parte del registro referida al accidente tiene los campos:\n",
    " - 'FECHA'\n",
    " - 'RANGO HORARIO'\n",
    " - 'DIA SEMANA'\n",
    " - 'DISTRITO'\n",
    " - 'LUGAR ACCIDENTE'\n",
    " - 'Nº: string'\n",
    " - 'Nº PARTE'     **Id.accidente, es el mismo para varios regs.**\n",
    " - 'CPFA Granizo'\n",
    " - 'CPFA Hielo'\n",
    " - 'CPFA Lluvia'\n",
    " - 'CPFA Niebla'\n",
    " - 'CPFA Seco'\n",
    " - 'CPFA Nieve'\n",
    " - 'CPSV Mojada'\n",
    " - 'CPSV Aceite'\n",
    " - 'CPSV Barro'\n",
    " - 'CPSV Grava Suelta'\n",
    " - 'CPSV Hielo'\n",
    " - 'CPSV Seca Y Limpia'\n",
    " - '* Nº VICTIMAS' **Total de víctimas por Nº de parte**\n",
    " - 'TIPO ACCIDENTE'\n",
    " - 'Tipo Vehiculo'\n",
    " \n",
    "### La parte del registro correspondiente a cada persona:\n",
    " - 'TIPO PERSONA'\n",
    " - 'SEXO'\n",
    " - 'LESIVIDAD'\n",
    " - 'Tramo Edad'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext created\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"bubbly\"\n",
    "master = \"local[*]\"\n",
    "spark = (SparkSession.builder\n",
    "    .master(master)\n",
    "    .config(\"spark.driver.cores\", 1)\n",
    "    .appName(app_name)\n",
    "    .getOrCreate() )\n",
    "sc = spark.sparkContext\n",
    "print ('SparkContext created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el fichero desde hdfs\n",
    "accidenteData = spark.read.csv ('hdfs://localhost:9000/user/ubuntu/accidentes/2018.csv',header=True)\n",
    "#accidenteData = spark.read.csv ('file:///home/ubuntu/Downloads/2018_Accidentalidad.csv',header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "# Reducimos las columnas que tenemos que utilizar mediante la Select\n",
    "# y aplicamos las funciones necesarias a los datos\n",
    "accRed=accidenteData.select(\n",
    "#        func.to_date(accidenteData.FECHA, 'dd/MM/yyyy HH:mm:ss').alias('fecha'), \\\n",
    "        func.to_date(accidenteData.FECHA, 'dd/MM/yyyy').alias('fecha'), \\\n",
    "        accidenteData.DISTRITO.alias('distrito'), \\\n",
    "        accidenteData[\"Nº PARTE\"].alias('idAccidente'), \\\n",
    "        func.substring(accidenteData.LESIVIDAD,1,2).alias('lesividad'), \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- distrito: string (nullable = true)\n",
      " |-- idAccidente: string (nullable = true)\n",
      " |-- lesividad: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Las columnas del Dataframe accRed quedan así\n",
    "accRed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las filas de las personas ilesas\n",
    "# Agrupamos por fecha, distrito, accidente y lesividad para contar \n",
    "# los registros y tener el total de afectados.\n",
    "accGrouped=accRed.filter(accRed.lesividad != \"IL\")\\\n",
    "                .groupBy('fecha','distrito','idAccidente','lesividad')\\\n",
    "                .agg(func.count('lesividad').alias('victimas'))\\\n",
    "                .sort ('fecha','distrito','idAccidente','lesividad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- distrito: string (nullable = true)\n",
      " |-- idAccidente: string (nullable = true)\n",
      " |-- lesividad: string (nullable = true)\n",
      " |-- victimas: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Las columnas del Dataframe accRed quedan así\n",
    "accGrouped.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Por comodidad para poder guardar todos los datos en un único fichero utilizamos la librería pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accGrouped.toPandas().to_csv('/home/ubuntu/Downloads/victimas_por_fecha_distrito_accidente_lesion.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
